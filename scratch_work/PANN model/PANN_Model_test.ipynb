{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import librosa\n",
    "\n",
    "file_paths = glob.glob(\"E:/Users/atag3/Documents/Gunshot_Data/*.wav\")\n",
    "labels = [os.path.basename(x) for x in glob.glob('E:/Users/atag3/Documents/Gunshot_Data/*.wav')]\n",
    "\n",
    "dat = {'file_path':file_paths,'label':labels}\n",
    "gun_dat = pd.DataFrame(dat)\n",
    "\n",
    "\n",
    "sample_rate = 32000\n",
    "classes_num =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_utils'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5652cd5dc2b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugmentation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSpecAugmentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpytorch_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdo_mixup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_framewise_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mWavegram_Logmel_Cnn14\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_utils'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "\n",
    "from pytorch_utils import do_mixup, interpolate, pad_framewise_output\n",
    "\n",
    "class Wavegram_Logmel_Cnn14(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num):\n",
    "        \n",
    "        super(Wavegram_Logmel_Cnn14, self).__init__()\n",
    "\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = None\n",
    "\n",
    "        self.pre_conv0 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=11, stride=5, padding=5, bias=False)\n",
    "        self.pre_bn0 = nn.BatchNorm1d(64)\n",
    "        self.pre_block1 = ConvPreWavBlock(64, 64)\n",
    "        self.pre_block2 = ConvPreWavBlock(64, 128)\n",
    "        self.pre_block3 = ConvPreWavBlock(128, 128)\n",
    "        self.pre_block4 = ConvBlock(in_channels=4, out_channels=64)\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
    "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
    "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
    "            freeze_parameters=True)\n",
    "\n",
    "        # Spec augmenter\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
    "            freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
    "        self.conv_block2 = ConvBlock(in_channels=128, out_channels=128)\n",
    "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
    "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
    "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
    "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
    "        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_layer(self.pre_conv0)\n",
    "        init_bn(self.pre_bn0)\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        init_layer(self.fc_audioset)\n",
    " \n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "\n",
    "        # Wavegram\n",
    "        a1 = F.relu_(self.pre_bn0(self.pre_conv0(input[:, None, :])))\n",
    "        a1 = self.pre_block1(a1, pool_size=4)\n",
    "        a1 = self.pre_block2(a1, pool_size=4)\n",
    "        a1 = self.pre_block3(a1, pool_size=4)\n",
    "        a1 = a1.reshape((a1.shape[0], -1, 32, a1.shape[-1])).transpose(2, 3)\n",
    "        a1 = self.pre_block4(a1, pool_size=(2, 1))\n",
    "\n",
    "        # Log mel spectrogram\n",
    "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
    "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        if self.training:\n",
    "            x = self.spec_augmenter(x)\n",
    "\n",
    "        # Mixup on spectrogram\n",
    "        if self.training and mixup_lambda is not None:\n",
    "            x = do_mixup(x, mixup_lambda)\n",
    "            a1 = do_mixup(a1, mixup_lambda)\n",
    "        \n",
    "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
    "\n",
    "        # Concatenate Wavegram and Log mel spectrogram along the channel dimension\n",
    "        x = torch.cat((x, a1), dim=1)\n",
    "\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        \n",
    "        (x1, _) = torch.max(x, dim=2)\n",
    "        x2 = torch.mean(x, dim=2)\n",
    "        x = x1 + x2\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        embedding = F.dropout(x, p=0.5, training=self.training)\n",
    "        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n",
    "        \n",
    "        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "            \n",
    "    \n",
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(3, 3), stride=(1, 1),\n",
    "                              padding=(1, 1), bias=False)\n",
    "                              \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "        \n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "        \n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        if pool_type == 'max':\n",
    "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg+max':\n",
    "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
    "            x = x1 + x2\n",
    "        else:\n",
    "            raise Exception('Incorrect argument!')\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvBlock5x5(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super(ConvBlock5x5, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(5, 5), stride=(1, 1),\n",
    "                              padding=(2, 2), bias=False)\n",
    "                              \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_bn(self.bn1)\n",
    "\n",
    "        \n",
    "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
    "        \n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        if pool_type == 'max':\n",
    "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg':\n",
    "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "        elif pool_type == 'avg+max':\n",
    "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
    "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
    "            x = x1 + x2\n",
    "        else:\n",
    "            raise Exception('Incorrect argument!')\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class AttBlock(nn.Module):\n",
    "    def __init__(self, n_in, n_out, activation='linear', temperature=1.):\n",
    "        super(AttBlock, self).__init__()\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.temperature = temperature\n",
    "        self.att = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.cla = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        \n",
    "        self.bn_att = nn.BatchNorm1d(n_out)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "        init_bn(self.bn_att)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "    \n",
    "class ConvPreWavBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super(ConvPreWavBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, \n",
    "                            out_channels=out_channels,\n",
    "                            kernel_size=3, stride=1,\n",
    "                            padding=1, bias=False)\n",
    "                            \n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channels, \n",
    "                            out_channels=out_channels,\n",
    "                            kernel_size=3, stride=1, dilation=2, \n",
    "                            padding=2, bias=False)\n",
    "                            \n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "\n",
    "        \n",
    "    def forward(self, input, pool_size):\n",
    "        \n",
    "        x = input\n",
    "        x = F.relu_(self.bn1(self.conv1(x)))\n",
    "        x = F.relu_(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool1d(x, kernel_size=pool_size)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Wavegram_Logmel_Cnn14:\n\tMissing key(s) in state_dict: \"pre_conv0.weight\", \"pre_bn0.weight\", \"pre_bn0.bias\", \"pre_bn0.running_mean\", \"pre_bn0.running_var\", \"pre_block1.conv1.weight\", \"pre_block1.conv2.weight\", \"pre_block1.bn1.weight\", \"pre_block1.bn1.bias\", \"pre_block1.bn1.running_mean\", \"pre_block1.bn1.running_var\", \"pre_block1.bn2.weight\", \"pre_block1.bn2.bias\", \"pre_block1.bn2.running_mean\", \"pre_block1.bn2.running_var\", \"pre_block2.conv1.weight\", \"pre_block2.conv2.weight\", \"pre_block2.bn1.weight\", \"pre_block2.bn1.bias\", \"pre_block2.bn1.running_mean\", \"pre_block2.bn1.running_var\", \"pre_block2.bn2.weight\", \"pre_block2.bn2.bias\", \"pre_block2.bn2.running_mean\", \"pre_block2.bn2.running_var\", \"pre_block3.conv1.weight\", \"pre_block3.conv2.weight\", \"pre_block3.bn1.weight\", \"pre_block3.bn1.bias\", \"pre_block3.bn1.running_mean\", \"pre_block3.bn1.running_var\", \"pre_block3.bn2.weight\", \"pre_block3.bn2.bias\", \"pre_block3.bn2.running_mean\", \"pre_block3.bn2.running_var\", \"pre_block4.conv1.weight\", \"pre_block4.conv2.weight\", \"pre_block4.bn1.weight\", \"pre_block4.bn1.bias\", \"pre_block4.bn1.running_mean\", \"pre_block4.bn1.running_var\", \"pre_block4.bn2.weight\", \"pre_block4.bn2.bias\", \"pre_block4.bn2.running_mean\", \"pre_block4.bn2.running_var\", \"spectrogram_extractor.stft.conv_real.weight\", \"spectrogram_extractor.stft.conv_imag.weight\", \"logmel_extractor.melW\", \"bn0.weight\", \"bn0.bias\", \"bn0.running_mean\", \"bn0.running_var\", \"conv_block1.conv1.weight\", \"conv_block1.conv2.weight\", \"conv_block1.bn1.weight\", \"conv_block1.bn1.bias\", \"conv_block1.bn1.running_mean\", \"conv_block1.bn1.running_var\", \"conv_block1.bn2.weight\", \"conv_block1.bn2.bias\", \"conv_block1.bn2.running_mean\", \"conv_block1.bn2.running_var\", \"conv_block2.conv1.weight\", \"conv_block2.conv2.weight\", \"conv_block2.bn1.weight\", \"conv_block2.bn1.bias\", \"conv_block2.bn1.running_mean\", \"conv_block2.bn1.running_var\", \"conv_block2.bn2.weight\", \"conv_block2.bn2.bias\", \"conv_block2.bn2.running_mean\", \"conv_block2.bn2.running_var\", \"conv_block3.conv1.weight\", \"conv_block3.conv2.weight\", \"conv_block3.bn1.weight\", \"conv_block3.bn1.bias\", \"conv_block3.bn1.running_mean\", \"conv_block3.bn1.running_var\", \"conv_block3.bn2.weight\", \"conv_block3.bn2.bias\", \"conv_block3.bn2.running_mean\", \"conv_block3.bn2.running_var\", \"conv_block4.conv1.weight\", \"conv_block4.conv2.weight\", \"conv_block4.bn1.weight\", \"conv_block4.bn1.bias\", \"conv_block4.bn1.running_mean\", \"conv_block4.bn1.running_var\", \"conv_block4.bn2.weight\", \"conv_block4.bn2.bias\", \"conv_block4.bn2.running_mean\", \"conv_block4.bn2.running_var\", \"conv_block5.conv1.weight\", \"conv_block5.conv2.weight\", \"conv_block5.bn1.weight\", \"conv_block5.bn1.bias\", \"conv_block5.bn1.running_mean\", \"conv_block5.bn1.running_var\", \"conv_block5.bn2.weight\", \"conv_block5.bn2.bias\", \"conv_block5.bn2.running_mean\", \"conv_block5.bn2.running_var\", \"conv_block6.conv1.weight\", \"conv_block6.conv2.weight\", \"conv_block6.bn1.weight\", \"conv_block6.bn1.bias\", \"conv_block6.bn1.running_mean\", \"conv_block6.bn1.running_var\", \"conv_block6.bn2.weight\", \"conv_block6.bn2.bias\", \"conv_block6.bn2.running_mean\", \"conv_block6.bn2.running_var\", \"fc1.weight\", \"fc1.bias\", \"fc_audioset.weight\", \"fc_audioset.bias\". \n\tUnexpected key(s) in state_dict: \"iteration\", \"model\", \"optimizer\", \"sampler\". ",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3c7544f8261e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWavegram_Logmel_Cnn14\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m44100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11025\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Wavegram_Logmel_Cnn14.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1052\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1053\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Wavegram_Logmel_Cnn14:\n\tMissing key(s) in state_dict: \"pre_conv0.weight\", \"pre_bn0.weight\", \"pre_bn0.bias\", \"pre_bn0.running_mean\", \"pre_bn0.running_var\", \"pre_block1.conv1.weight\", \"pre_block1.conv2.weight\", \"pre_block1.bn1.weight\", \"pre_block1.bn1.bias\", \"pre_block1.bn1.running_mean\", \"pre_block1.bn1.running_var\", \"pre_block1.bn2.weight\", \"pre_block1.bn2.bias\", \"pre_block1.bn2.running_mean\", \"pre_block1.bn2.running_var\", \"pre_block2.conv1.weight\", \"pre_block2.conv2.weight\", \"pre_block2.bn1.weight\", \"pre_block2.bn1.bias\", \"pre_block2.bn1.running_mean\", \"pre_block2.bn1.running_var\", \"pre_block2.bn2.weight\", \"pre_block2.bn2.bias\", \"pre_block2.bn2.running_mean\", \"pre_block2.bn2.running_var\", \"pre_block3.conv1.weight\", \"pre_block3.conv2.weight\", \"pre_block3.bn1.weight\", \"pre_block3.bn1.bias\", \"pre_block3.bn1.running_mean\", \"pre_block3.bn1.running_var\", \"pre_block3.bn2.weight\", \"pre_block3.bn2.bias\", \"pre_block3.bn2.running_mean\", \"pre_block3.bn2.running_var\", \"pre_block4.conv1.weight\", \"pre_block4.conv2.weight\", \"pre_block4.bn1.weight\", \"pre_block4.bn1.bias\", \"pre_block4.bn1.running_mean\", \"pre_block4.bn1.running_var\", \"pre_block4.bn2.weight\", \"pre_block4.bn2.bias\", \"pre_block4.bn2.running_mean\", \"pre_block4.bn2.running_var\", \"spectrogram_extractor.stft.conv_real.weight\", \"spectrogram_extractor.stft.conv_imag.weight\", \"logmel_extractor.melW\", \"bn0.weight\", \"bn0.bias\", \"bn0.running_mean\", \"bn0.running_var\", \"conv_block1.conv1.weight\", \"conv_block1.conv2.weight\", \"conv_block1.bn1.weight\", \"conv_block1.bn1.bias\", \"conv_block1.bn1.running_mean\", \"conv_block1.bn1.running_var\", \"conv_block1.bn2.weight\", \"conv_block1.bn2.bias\", \"conv_block1.bn2.running_mean\", \"conv_block1.bn2.running_var\", \"conv_block2.conv1.weight\", \"conv_block2.conv2.weight\", \"conv_block2.bn1.weight\", \"conv_block2.bn1.bias\", \"conv_block2.bn1.running_mean\", \"conv_block2.bn1.running_var\", \"conv_block2.bn2.weight\", \"conv_block2.bn2.bias\", \"conv_block2.bn2.running_mean\", \"conv_block2.bn2.running_var\", \"conv_block3.conv1.weight\", \"conv_block3.conv2.weight\", \"conv_block3.bn1.weight\", \"conv_block3.bn1.bias\", \"conv_block3.bn1.running_mean\", \"conv_block3.bn1.running_var\", \"conv_block3.bn2.weight\", \"conv_block3.bn2.bias\", \"conv_block3.bn2.running_mean\", \"conv_block3.bn2.running_var\", \"conv_block4.conv1.weight\", \"conv_block4.conv2.weight\", \"conv_block4.bn1.weight\", \"conv_block4.bn1.bias\", \"conv_block4.bn1.running_mean\", \"conv_block4.bn1.running_var\", \"conv_block4.bn2.weight\", \"conv_block4.bn2.bias\", \"conv_block4.bn2.running_mean\", \"conv_block4.bn2.running_var\", \"conv_block5.conv1.weight\", \"conv_block5.conv2.weight\", \"conv_block5.bn1.weight\", \"conv_block5.bn1.bias\", \"conv_block5.bn1.running_mean\", \"conv_block5.bn1.running_var\", \"conv_block5.bn2.weight\", \"conv_block5.bn2.bias\", \"conv_block5.bn2.running_mean\", \"conv_block5.bn2.running_var\", \"conv_block6.conv1.weight\", \"conv_block6.conv2.weight\", \"conv_block6.bn1.weight\", \"conv_block6.bn1.bias\", \"conv_block6.bn1.running_mean\", \"conv_block6.bn1.running_var\", \"conv_block6.bn2.weight\", \"conv_block6.bn2.bias\", \"conv_block6.bn2.running_mean\", \"conv_block6.bn2.running_var\", \"fc1.weight\", \"fc1.bias\", \"fc_audioset.weight\", \"fc_audioset.bias\". \n\tUnexpected key(s) in state_dict: \"iteration\", \"model\", \"optimizer\", \"sampler\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = Wavegram_Logmel_Cnn14(44100, 2048, 512, 128, 0,11025,3)\n",
    "model.load_state_dict(torch.load('Wavegram_Logmel_Cnn14.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = torch.load('Wavegram_Logmel_Cnn14.pth')\n",
    "model = Wavegram_Logmel_Cnn14(44100, 2048, 512, 128, 0,11025,3)\n",
    "#model.load_state_dict(pretrained_model,strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-8559337f0fde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-88b1a977691b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, mixup_lambda)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# Wavegram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_bn0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_conv0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_block1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_block2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "audio, sample_rate = librosa.load(file_paths[0]) \n",
    "model(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "eval() arg 1 must be a string, bytes or code object",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-ea4d494daef8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msound_event_detection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-50f7b324b8a7>\u001b[0m in \u001b[0;36msound_event_detection\u001b[1;34m(audio_path)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     model = Model(sample_rate=sample_rate, window_size=window_size, \n\u001b[0;32m     28\u001b[0m         \u001b[0mhop_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhop_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmel_bins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmel_bins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfmax\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: eval() arg 1 must be a string, bytes or code object"
     ]
    }
   ],
   "source": [
    "sound_event_detection(file_paths[0])"
   ]
  }
 ]
}